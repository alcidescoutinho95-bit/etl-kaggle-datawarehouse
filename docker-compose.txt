# version: '3.8'

x-spark-common: &spark-common
  image: wlcamargo/spark-master
  networks:
    - sparkanos

services: 
  # ----------------
  # Postgres
  # ----------------
  postgres:
    hostname: postgres
    image: postgres:14
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks: 
      - sparkanos

  # ----------------
  # MinIO
  # ----------------
  minio: 
    hostname: minio 
    container_name: minio 
    image: 'minio/minio:RELEASE.2024-01-13T07-53-03Z' 
    ports: 
      - '9000:9000' 
      - '9001:9001' 
    volumes: 
      - minio:/data 
    environment: 
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001" 
    networks: 
      - sparkanos
 
  # ----------------
  # Spark
  # ----------------
  spark-master:
    <<: *spark-common
    hostname: spark-master
    container_name: spark-master
    command: 
      - /bin/sh
      - -c
      - |
        if [ -f /tmp/requirements.txt ]; then
          echo "Instalando dependências Python..."
          pip install --no-cache-dir -r /tmp/requirements.txt
        fi
        /usr/local/spark/sbin/start-master.sh
        start-notebook.sh --NotebookApp.token=''
    volumes:
      - ./conf/util:/util
      - ./conf/env:/env
      - ./src/Notebooks:/home/user/work
      - ./datasets:/home/user/datasets
      - ./requirements.txt:/tmp/requirements.txt:ro
      - C:/Users/senai/Downloads/kaggle.json:/user/.kaggle/kaggle.json:ro
    environment:
      KAGGLE_CONFIG_DIR: /user/.kaggle
    ports:
      - 8889:8888 # Jupyter
      - 8083:8080 # Spark UI
      - 4040:4040 # Spark UI Jobs
      - 7077:7077 # Default port for communication

  # ----------------
  # Airflow infra
  # ----------------
  redis:
    image: redis:latest
    networks: 
      - sparkanos

  # Serviço para inicialização do banco
  airflow-init:
    image: apache/airflow:2.9.1
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-logs:/opt/airflow/logs
    command: >
      bash -c "
      echo 'Aguardando PostgreSQL ficar pronto...'
      sleep 10
      echo 'Recriando banco completamente...'
      PGPASSWORD=airflow psql -h postgres -U airflow -d postgres -c 'DROP DATABASE IF EXISTS airflow;'
      PGPASSWORD=airflow psql -h postgres -U airflow -d postgres -c 'CREATE DATABASE airflow;'
      echo 'Inicializando Airflow...'
      airflow db migrate
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      echo 'Airflow inicializado com sucesso!'"
    depends_on:
      - postgres
      - redis
    networks: 
      - sparkanos
  airflow-webserver:
      image: apache/airflow:2.9.1
      ports:
        - "8081:8080"
      environment:
        - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
        - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
        - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
      volumes:
        - ./dags:/opt/airflow/dags
        - ./airflow-logs:/opt/airflow/logs
      command: webserver
      depends_on:
        - airflow-init
        - postgres
        - redis
      networks: 
        - sparkanos

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.9.1
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-logs:/opt/airflow/logs
    depends_on:
      - airflow-init
    networks: 
      - sparkanos

  # Airflow Worker
  airflow-worker:
    image: apache/airflow:2.9.1
    command: celery worker
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-logs:/opt/airflow/logs
    depends_on:
      - airflow-init
    networks: 
      - sparkanos

# ----------------
# Networks & Volumes
# ----------------
networks: 
  sparkanos: 
    driver: bridge 

volumes: 
  minio:
  postgres_data:
  airflow-logs: